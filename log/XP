File Name: XP

XP Style

Extreme Programming (XP) is a discipline of software development based on values of 
simplicity, communication, feedback, and courage. It works by bringing the whole team 
together in the presence of simple practices, with enough feedback to enable the team 
to see where they are and to tune the practices to their unique situation.

============================
(1) Basics
============================

* Extreme Programming is one of several popular Agile Processes.
* The team forms around a business representative called the `Customer'.
* XP teams follow a common coding standard. 
* All the code looks as if it was written by a single very competent individual.
* Code in a consistent style so that everyone can understand and improve the code.

============================
(2) Planning
============================

* XP programmers do Continuous Integration and frequent small releases.
* Predict what will be accomplished by the due date, and determine what to do next.
* Deliver the software you need as you need it.
* A feature was completed, or it was not -- No 80% finished!
* Create your tests first, before the actual code!

============================
(3) Pair Programming
============================

* Extreme Programmers work together in pairs and as a group.
* Each group has two persons: a programmer and a writer -- pair programming.
* Two heads really are better than one! -- much better! 
* Programmers write the code and writers check them. 
* Writers document the code and programmers make sure it reflects the functioning.

============================
(4) Documentation
============================

* Almost no one reads the manual. So keep it small and simple!
* Documenting a feature shouldn't be any more difficult than implementing it.
* Plan to have documentation one iteration behind development.


References:

[1] eXtreme Programming (XP): http://xprogramming.com/
[2] Unit test: http://check.sourceforge.net/
[3] Refactoring code: http://martinfowler.com/refactoring/
[4] Agile process: http://www.agile-process.org/


=======================================
Appendix A: Some General Guidelines
=======================================

* Follow the data structures defined in fasp.h. Use the defined data structures as much 
  as possible for public functions. Use lower-level data structures for private functions
  for efficiency.
 
* Always initialize a variable when declaring it if possible.

* Avoid allocate and free memory repetitively. Think about open a temp array instead. 
  
* Always check memory allocation. 

* Alway use meaningful variable names. Variables should be as local as possible. 

* Define variables in the beginning of C-files before executable lines. This is for 
  compiler compatibility and make the codes more readable. You may break this rule for 
  very local variables.

* When write a long "if" or "for", always close "}" with comment like " // end of XXX"
  to make the code clear. 

* Define local/private functions as "static". This is important for automatic header 
  generator. 

* Profile your code in order to find the bottle necks.

* Subroutines are re-usable! Reduce repetition as much as possible. Consider to make 
  small and heavily-used functions inline or static.

* Minimize the use of copy-and-paste. That's the reason for duplications! Before we 
  copy-and-paste a section of code, think about write a subroutine instead. 

* Keep track of things by adding version info in the beginning of each function. 


=======================================
Appendix B: Some Helpful Tips
=======================================

WARNING: These tips were collected from the Internet. 
WARNING: Some of them might become obsolete.
WARNING: Use them with caution!

#1. Declare as static functions that are not used outside the file where they are defined

Declaring a function as static forces an internal linkage within that file, which can 
improve the performance of the code. Functions that are not declared as static default 
to external linkage, which may inhibit some optimizations, such as aggressive inlining, 
with some C/C++ compilers.

#2: Use array notation instead of pointer notation when working with arrays

You can use either array operators ([]) or pointers to access the elements of an array. 
However, it's often hard for the compiler to optimize pointers because the pointers are, 
by default, less constrained. The compiler has to assume that the pointer can read/write 
to any location in memory. Because array notation is less ambiguous, the compiler may be 
able to optimize it more effectively. Because it's hard to know which way is more 
efficient, you may want to try coding your array reference using both pointers and array 
notation, and use a code profiler to see which way is fastest for your particular 
application.

#3. Completely unroll loops that have a small fixed loop count and a small loop body

As you know, many compilers do not aggressively unroll loops. Manually unrolling loops 
can benefit performance, because you're avoiding the loop overhead; for a small loop, 
that overhead can be significant.

For example, avoid a small loop like this:

// 3D-transform: Multiply vector V by 4x4 transform matrix M.
for (i = 0; i < 4; i++) {
   r[i] = 0;
   for (j = 0; j < 4; j++) {
      r[i] += m[j][i] * v[j];
   }
}

Instead, replace it with its completely unrolled equivalent, as shown here:

r[0] = m[0][0] * v[0] + m[1][0] * v[1] + m[2][0] * v[2] + m[3][0] * v[3];
r[1] = m[0][1] * v[0] + m[1][1] * v[1] + m[2][1] * v[2] + m[3][1] * v[3];
r[2] = m[0][2] * v[0] + m[1][2] * v[1] + m[2][2] * v[2] + m[3][2] * v[3];
r[3] = m[0][3] * v[0] + m[1][3] * v[1] + m[2][3] * v[2] + m[3][3] * v[3];

#4. Consider expression order when coding a compound branch

Complex branches which consist of many Boolean expressions with logical AND (&&) and 
logical OR (||) operators provide great opportunities for optimization, if you know 
which AND and OR conditions are more likely to occur.

That's because the C/C++ compiler will optimize the code to terminate execution of that 
complex branch as soon as it's sure if the operand evaluates to true or false, depending 
on the logic. For example, the branch conditions are satisfied as soon as it detects a 
true operand in an OR expression; similarly, the application knows that the branch will 
fail as soon as it sees a false operand in an AND expression.

You might experiment with changing the order of the operands if you know that a true or 
false outcome is more probable for a specific operand. For example, in an AND expression,
place the operand most likely to be false up front. Of course, you'll need to make sure 
that changing these order of the operands won't cause any unwanted side effects.

#5. Leverage the early termination abilities of complex If statements

As seen above, as soon as a true operand is found in an OR expression, or a false one is 
found in an AND expression, execution of that particular expression is terminated because
outcome is known. You can leverage that, if you have a good idea as to whether an entire
expression is likely to be true or false, to try to short-cut execution of the condition.

For example, say you're testing to see if a and b are going to be within a specified 
numeric range. If you know that those numbers will generally be within that range, you 
can use the following because the most likely outcome will evaluate to false and thereby 
terminate the AND condition:

  if (a <= max && a >= min && b <= max && b >= min)

If you know that the data will generally not be in a specified range, you can logically 
reverse the If statement to optimize execution in the same way, with a true terminating 
the OR condition:

  if (a > max || a < min || b > max || b < min)

#6. Use if-else in place of switch that have noncontiguous case expressions

If the case expressions are contiguous or nearly contiguous values, most compilers 
translate the switch statement as a jump table instead of a comparison chain. Jump tables 
generally improve performance because they reduce the number of branches to a single 
procedure call, and shrink the size of the control-flow code no matter how many cases 
there are. The amount of control-flow code that the processor must execute is also the 
same for all values of the switch expression.

However, if the case expressions are noncontiguous values, most compilers translate the 
switch statement as a comparison chain. Comparison chains are undesirable because they 
use dense sequences of conditional branches, which interfere with the processor's ability
to successfully perform branch prediction. Also, the amount of control-flow code increases
with the number of cases, and the amount of control-flow code that the processor must 
execute varies with the value of the switch expression.

For example, if the case expression are contiguous integers, a switch statement can 
provide good performance:

switch (grade)
{
   case 'A':
      ...
      break;
   case 'B':
      ...
      break;
   case 'C':
      ...
      break;
   case 'D':
      ...
      break;
   case 'F':
      ...
      break;
}

But if the case expression aren't contiguous, the compiler may likely translate the code
into a comparison chain instead of a jump table, and that can be slow:

switch (a)
{
   case 8:
      // Sequence for a==8
      break;
   case 16:
      // Sequence for a==16
      break;
   ...
   default:
      // Default sequence
      break;
}

In cases like this, replace the switch with a series of if-else statements:

if (a==8) {
   // Sequence for a==8
}
else if (a==16) {
   // Sequence for a==16
}
...
else {
   // Default sequence
}

#9. Restructure floating-point math to reduce the number of operations, if possible

Floating point operations have long latencies, even when you're using x87 (in 32-bit 
mode) or SSE/SSE instruction set extensions (in 32-bit or 64-bit mode), so those types 
of operations should be a target for optimization, particularly if you take into 
consideration the size of the AMD Opteron or Athlon64 processors' instruction pipeline.

However, you should be careful because even when algebraic rules would permit you to 
transform one floating-point operation into another, the algorithms within a 
microprocessor may not yield exactly the same results down to the least significant bits.
You should consult a book on numerical analysis or experiment to make sure that it's okay
to tinker with the math.

Here's an optimization example that involves the concept of data dependencies. This 
example, recommend by AMD, uses four-way unrolling to exploit the four-stage fully 
pipelined floating-point adder. Each stage of the floating-point adder is occupied on 
every clock cycle, ensuring maximum sustained utilization. Mathematically, it's 
equivalent; computationally, it's faster.

The original code:

double a[100], sum;
int i;
sum = 0.0f;
for (i = 0; i < 100; i++) {
   sum += a[i];
}

This version is faster, because the code implements four separate dependence chains 
instead of just one, which keeps the pipeline full. The /fp:fast compiler switch in 
Visual Studio 2005 can help do this sort of optimization automatically.

double a[100], sum1, sum2, sum3, sum4, sum;
int i;
sum1 = 0.0;
sum2 = 0.0;
sum3 = 0.0;
sum4 = 0.0;
for (i = 0; i < 100; i + 4) {
   sum1 += a[i];
   sum2 += a[i+1];
   sum3 += a[i+2];
   sum4 += a[i+3];
}
sum = (sum4 + sum3) + (sum1 + sum2);

#10. Use 32-bit integers instead of 8-bit or 16-bit integers

This is a tip specifically for 32-bit applications running either natively on a 32-bit 
operating system, or within a 64-bit operating system. You'll see better performance 
because operations on 8-bit or 16-bit integers are often less efficient. Of course, 
you'll be using twice or four times the memory, so be aware of that trade-off.

By the way, 32-bit integers execute at full speed when you're running in 64-bit mode, so 
unless you need the extra bits for some application-specific reason, you should stick to 
32-bit ones.
